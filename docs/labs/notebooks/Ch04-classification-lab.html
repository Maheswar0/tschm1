

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 4 &#8212; Introduction to Statistical Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/labs/notebooks/Ch04-classification-lab';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Chapter 5" href="Ch05-resample-lab.html" />
    <link rel="prev" title="Chapter 3" href="Ch03-linreg-lab.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo.jpeg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../_static/logo.jpeg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    An Introduction to Statistical Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Labs</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Ch02-statlearn-lab.html">Chapter 2</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch03-linreg-lab.html">Chapter 3</a></li>

<li class="toctree-l2 current active"><a class="current reference internal" href="#">Chapter 4</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch05-resample-lab.html">Chapter 5</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch06-varselect-lab.html">Chapter 6</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch07-nonlin-lab.html">Chapter 7</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch08-baggboost-lab.html">Chapter 8</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch09-svm-lab.html">Chapter 9</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch10-deeplearning-lab.html">Chapter 10</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch11-surv-lab.html">Chapter 11</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch12-unsup-lab.html">Chapter 12</a></li>

<li class="toctree-l2"><a class="reference internal" href="Ch13-multiple-lab.html">Chapter 13</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../slides/index.html">Slides</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/tschm/isl/master?urlpath=tree/book/docs/labs/notebooks/Ch04-classification-lab.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tschm/isl" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tschm/isl/edit/master/book/docs/labs/notebooks/Ch04-classification-lab.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tschm/isl/issues/new?title=Issue%20on%20page%20%2Fdocs/labs/notebooks/Ch04-classification-lab.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/docs/labs/notebooks/Ch04-classification-lab.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 4</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 4</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-logistic-regression-lda-qda-and-knn">Lab: Logistic Regression, LDA, QDA, and KNN</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-stock-market-data">The Stock Market Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-discriminant-analysis">Quadratic Discriminant Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">K-Nearest Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-parameters">Tuning Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-to-logistic-regression">Comparison to Logistic Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-and-poisson-regression-on-the-bikeshare-data">Linear and Poisson Regression on the Bikeshare Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">Poisson Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-4">
<h1>Chapter 4<a class="headerlink" href="#chapter-4" title="Permalink to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="lab-logistic-regression-lda-qda-and-knn">
<h1>Lab: Logistic Regression, LDA, QDA, and KNN<a class="headerlink" href="#lab-logistic-regression-lda-qda-and-knn" title="Permalink to this heading">#</a></h1>
<section id="the-stock-market-data">
<h2>The Stock Market Data<a class="headerlink" href="#the-stock-market-data" title="Permalink to this heading">#</a></h2>
<p>In this lab we will examine the <code class="docutils literal notranslate"><span class="pre">Smarket</span></code>
data, which is part of the <code class="docutils literal notranslate"><span class="pre">ISLP</span></code>
library. This data set consists of percentage returns for the S&amp;P 500
stock index over 1,250 days, from the beginning of 2001 until the end
of 2005. For each date, we have recorded the percentage returns for
each of the five previous trading days,  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  through
<code class="docutils literal notranslate"><span class="pre">Lag5</span></code>. We have also recorded  <code class="docutils literal notranslate"><span class="pre">Volume</span></code>  (the number of
shares traded on the previous day, in billions),  <code class="docutils literal notranslate"><span class="pre">Today</span></code>  (the
percentage return on the date in question) and  <code class="docutils literal notranslate"><span class="pre">Direction</span></code>
(whether the market was  <code class="docutils literal notranslate"><span class="pre">Up</span></code>  or  <code class="docutils literal notranslate"><span class="pre">Down</span></code>  on this date).</p>
<p>We start by importing  our libraries at this top level; these are all imports we have seen in previous labs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">import</span> <span class="n">subplots</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">ISLP</span> <span class="kn">import</span> <span class="n">load_data</span>
<span class="kn">from</span> <span class="nn">ISLP.models</span> <span class="kn">import</span> <span class="p">(</span><span class="n">ModelSpec</span> <span class="k">as</span> <span class="n">MS</span><span class="p">,</span>
                         <span class="n">summarize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We also collect together the new imports needed for this lab.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ISLP</span> <span class="kn">import</span> <span class="n">confusion_table</span>
<span class="kn">from</span> <span class="nn">ISLP.models</span> <span class="kn">import</span> <span class="n">contrast</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> \
     <span class="p">(</span><span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span><span class="p">,</span>
      <span class="n">QuadraticDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">QDA</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</div>
</div>
</div>
<p>Now we are ready to load the <code class="docutils literal notranslate"><span class="pre">Smarket</span></code> data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Smarket</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s1">&#39;Smarket&#39;</span><span class="p">)</span>
<span class="n">Smarket</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Year</th>
      <th>Lag1</th>
      <th>Lag2</th>
      <th>Lag3</th>
      <th>Lag4</th>
      <th>Lag5</th>
      <th>Volume</th>
      <th>Today</th>
      <th>Direction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2001</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>-2.624</td>
      <td>-1.055</td>
      <td>5.010</td>
      <td>1.19130</td>
      <td>0.959</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2001</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>-2.624</td>
      <td>-1.055</td>
      <td>1.29650</td>
      <td>1.032</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2001</td>
      <td>1.032</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>-2.624</td>
      <td>1.41120</td>
      <td>-0.623</td>
      <td>Down</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2001</td>
      <td>-0.623</td>
      <td>1.032</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>-0.192</td>
      <td>1.27600</td>
      <td>0.614</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2001</td>
      <td>0.614</td>
      <td>-0.623</td>
      <td>1.032</td>
      <td>0.959</td>
      <td>0.381</td>
      <td>1.20570</td>
      <td>0.213</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1245</th>
      <td>2005</td>
      <td>0.422</td>
      <td>0.252</td>
      <td>-0.024</td>
      <td>-0.584</td>
      <td>-0.285</td>
      <td>1.88850</td>
      <td>0.043</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>1246</th>
      <td>2005</td>
      <td>0.043</td>
      <td>0.422</td>
      <td>0.252</td>
      <td>-0.024</td>
      <td>-0.584</td>
      <td>1.28581</td>
      <td>-0.955</td>
      <td>Down</td>
    </tr>
    <tr>
      <th>1247</th>
      <td>2005</td>
      <td>-0.955</td>
      <td>0.043</td>
      <td>0.422</td>
      <td>0.252</td>
      <td>-0.024</td>
      <td>1.54047</td>
      <td>0.130</td>
      <td>Up</td>
    </tr>
    <tr>
      <th>1248</th>
      <td>2005</td>
      <td>0.130</td>
      <td>-0.955</td>
      <td>0.043</td>
      <td>0.422</td>
      <td>0.252</td>
      <td>1.42236</td>
      <td>-0.298</td>
      <td>Down</td>
    </tr>
    <tr>
      <th>1249</th>
      <td>2005</td>
      <td>-0.298</td>
      <td>0.130</td>
      <td>-0.955</td>
      <td>0.043</td>
      <td>0.422</td>
      <td>1.38254</td>
      <td>-0.489</td>
      <td>Down</td>
    </tr>
  </tbody>
</table>
<p>1250 rows × 9 columns</p>
</div></div></div>
</div>
<p>This gives a truncated listing of the data.
We can see what the variable names are.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Smarket</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;Year&#39;, &#39;Lag1&#39;, &#39;Lag2&#39;, &#39;Lag3&#39;, &#39;Lag4&#39;, &#39;Lag5&#39;, &#39;Volume&#39;, &#39;Today&#39;,
       &#39;Direction&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<p>We compute the correlation matrix using the <code class="docutils literal notranslate"><span class="pre">corr()</span></code> method
for data frames, which produces a matrix that contains all of
the pairwise correlations among the variables.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">pandas</span></code> library does not report a correlation for the <code class="docutils literal notranslate"><span class="pre">Direction</span></code>  variable because it is
qualitative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Smarket</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ValueError</span><span class="g g-Whitespace">                                </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/pandas/core/frame.py:10054,</span> in <span class="ni">DataFrame.corr</span><span class="nt">(self, method, min_periods, numeric_only)</span>
<span class="g g-Whitespace">  </span><span class="mi">10052</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span>
<span class="g g-Whitespace">  </span><span class="mi">10053</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">cols</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="ne">&gt; </span><span class="mi">10054</span> <span class="n">mat</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">na_value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">  </span><span class="mi">10056</span> <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;pearson&quot;</span><span class="p">:</span>
<span class="g g-Whitespace">  </span><span class="mi">10057</span>     <span class="n">correl</span> <span class="o">=</span> <span class="n">libalgos</span><span class="o">.</span><span class="n">nancorr</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">minp</span><span class="o">=</span><span class="n">min_periods</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/pandas/core/frame.py:1838,</span> in <span class="ni">DataFrame.to_numpy</span><span class="nt">(self, dtype, copy, na_value)</span>
<span class="g g-Whitespace">   </span><span class="mi">1836</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1837</span>     <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1838</span> <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mgr</span><span class="o">.</span><span class="n">as_array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span> <span class="n">na_value</span><span class="o">=</span><span class="n">na_value</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1839</span> <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">dtype</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1840</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1732,</span> in <span class="ni">BlockManager.as_array</span><span class="nt">(self, dtype, copy, na_value)</span>
<span class="g g-Whitespace">   </span><span class="mi">1730</span>         <span class="n">arr</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">writeable</span> <span class="o">=</span> <span class="kc">False</span>
<span class="g g-Whitespace">   </span><span class="mi">1731</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1732</span>     <span class="n">arr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_interleave</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">na_value</span><span class="o">=</span><span class="n">na_value</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1733</span>     <span class="c1"># The underlying data was copied within _interleave, so no need</span>
<span class="g g-Whitespace">   </span><span class="mi">1734</span>     <span class="c1"># to further copy if copy=True or setting na_value</span>
<span class="g g-Whitespace">   </span><span class="mi">1736</span> <span class="k">if</span> <span class="n">na_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">lib</span><span class="o">.</span><span class="n">no_default</span><span class="p">:</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1794,</span> in <span class="ni">BlockManager._interleave</span><span class="nt">(self, dtype, na_value)</span>
<span class="g g-Whitespace">   </span><span class="mi">1792</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1793</span>         <span class="n">arr</span> <span class="o">=</span> <span class="n">blk</span><span class="o">.</span><span class="n">get_values</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1794</span>     <span class="n">result</span><span class="p">[</span><span class="n">rl</span><span class="o">.</span><span class="n">indexer</span><span class="p">]</span> <span class="o">=</span> <span class="n">arr</span>
<span class="g g-Whitespace">   </span><span class="mi">1795</span>     <span class="n">itemmask</span><span class="p">[</span><span class="n">rl</span><span class="o">.</span><span class="n">indexer</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">1797</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">itemmask</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>

<span class="ne">ValueError</span>: could not convert string to float: &#39;Up&#39;
</pre></div>
</div>
</div>
</div>
<p>As one would expect, the correlations between the lagged return  variables and
today’s return are close to zero.  The only substantial correlation is between  <code class="docutils literal notranslate"><span class="pre">Year</span></code>  and
<code class="docutils literal notranslate"><span class="pre">Volume</span></code>. By plotting the data we see that  <code class="docutils literal notranslate"><span class="pre">Volume</span></code>
is increasing over time. In other words, the average number of shares traded
daily increased from 2001 to 2005.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Smarket</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Volume&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/4c3bae54b4a5673a7a0ac450f9e7dc6502113cd88dfdbd18fb224b25e5c30d0c.png" src="../../../_images/4c3bae54b4a5673a7a0ac450f9e7dc6502113cd88dfdbd18fb224b25e5c30d0c.png" />
</div>
</div>
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this heading">#</a></h2>
<p>Next, we will fit a logistic regression model in order to predict
<code class="docutils literal notranslate"><span class="pre">Direction</span></code>  using  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  through  <code class="docutils literal notranslate"><span class="pre">Lag5</span></code>  and
<code class="docutils literal notranslate"><span class="pre">Volume</span></code>. The <code class="docutils literal notranslate"><span class="pre">sm.GLM()</span></code>  function fits <em>generalized linear models</em>, a class of
models that includes logistic regression.  Alternatively,
the function <code class="docutils literal notranslate"><span class="pre">sm.Logit()</span></code> fits a logistic regression
model directly. The syntax of
<code class="docutils literal notranslate"><span class="pre">sm.GLM()</span></code> is similar to that of <code class="docutils literal notranslate"><span class="pre">sm.OLS()</span></code>, except
that we must pass in the argument <code class="docutils literal notranslate"><span class="pre">family=sm.families.Binomial()</span></code>
in order to tell <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> to run a logistic regression rather than some other
type of generalized linear model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">allvars</span> <span class="o">=</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Today&#39;</span><span class="p">,</span> <span class="s1">&#39;Direction&#39;</span><span class="p">,</span> <span class="s1">&#39;Year&#39;</span><span class="p">])</span>
<span class="n">design</span> <span class="o">=</span> <span class="n">MS</span><span class="p">(</span><span class="n">allvars</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">design</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Smarket</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">Direction</span> <span class="o">==</span> <span class="s1">&#39;Up&#39;</span>
<span class="n">glm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y</span><span class="p">,</span>
             <span class="n">X</span><span class="p">,</span>
             <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">glm</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">summarize</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coef</th>
      <th>std err</th>
      <th>z</th>
      <th>P&gt;|z|</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-0.1260</td>
      <td>0.241</td>
      <td>-0.523</td>
      <td>0.601</td>
    </tr>
    <tr>
      <th>Lag1</th>
      <td>-0.0731</td>
      <td>0.050</td>
      <td>-1.457</td>
      <td>0.145</td>
    </tr>
    <tr>
      <th>Lag2</th>
      <td>-0.0423</td>
      <td>0.050</td>
      <td>-0.845</td>
      <td>0.398</td>
    </tr>
    <tr>
      <th>Lag3</th>
      <td>0.0111</td>
      <td>0.050</td>
      <td>0.222</td>
      <td>0.824</td>
    </tr>
    <tr>
      <th>Lag4</th>
      <td>0.0094</td>
      <td>0.050</td>
      <td>0.187</td>
      <td>0.851</td>
    </tr>
    <tr>
      <th>Lag5</th>
      <td>0.0103</td>
      <td>0.050</td>
      <td>0.208</td>
      <td>0.835</td>
    </tr>
    <tr>
      <th>Volume</th>
      <td>0.1354</td>
      <td>0.158</td>
      <td>0.855</td>
      <td>0.392</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The smallest <em>p</em>-value here is associated with  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>. The
negative coefficient for this predictor suggests that if the market
had a positive return yesterday, then it is less likely to go up
today. However, at a value of 0.15, the <em>p</em>-value is still
relatively large, and so there is no clear evidence of a real
association between  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  and  <code class="docutils literal notranslate"><span class="pre">Direction</span></code>.</p>
<p>We use the <code class="docutils literal notranslate"><span class="pre">params</span></code>  attribute of <code class="docutils literal notranslate"><span class="pre">results</span></code>
in order to access just the
coefficients for this fitted model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>intercept   -0.126000
Lag1        -0.073074
Lag2        -0.042301
Lag3         0.011085
Lag4         0.009359
Lag5         0.010313
Volume       0.135441
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Likewise we can use the
<code class="docutils literal notranslate"><span class="pre">pvalues</span></code>  attribute to access the <em>p</em>-values for the coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span><span class="o">.</span><span class="n">pvalues</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>intercept    0.600700
Lag1         0.145232
Lag2         0.398352
Lag3         0.824334
Lag4         0.851445
Lag5         0.834998
Volume       0.392404
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">predict()</span></code>  method of <code class="docutils literal notranslate"><span class="pre">results</span></code> can be used to predict the
probability that the market will go up, given values of the
predictors. This method returns predictions
on the probability scale. If no data set is supplied to the <code class="docutils literal notranslate"><span class="pre">predict()</span></code>
function, then the probabilities are computed for the training data
that was used to fit the logistic regression model.
As with linear regression, one can pass an optional <code class="docutils literal notranslate"><span class="pre">exog</span></code> argument consistent
with a design matrix if desired. Here we have
printed only the first ten probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">probs</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.50708413, 0.48146788, 0.48113883, 0.51522236, 0.51078116,
       0.50695646, 0.49265087, 0.50922916, 0.51761353, 0.48883778])
</pre></div>
</div>
</div>
</div>
<p>In order to make a prediction as to whether the market will go up or
down on a particular day, we must convert these predicted
probabilities into class labels,  <code class="docutils literal notranslate"><span class="pre">Up</span></code>  or  <code class="docutils literal notranslate"><span class="pre">Down</span></code>.  The
following two commands create a vector of class predictions based on
whether the predicted probability of a market increase is greater than
or less than 0.5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;Down&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">1250</span><span class="p">)</span>
<span class="n">labels</span><span class="p">[</span><span class="n">probs</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;Up&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">confusion_table()</span></code>
function from the <code class="docutils literal notranslate"><span class="pre">ISLP</span></code> package summarizes these predictions, showing   how
many observations were correctly or incorrectly classified. Our function, which is adapted from a similar function
in the module <code class="docutils literal notranslate"><span class="pre">sklearn.metrics</span></code>,  transposes the resulting
matrix and includes row and column labels.
The <code class="docutils literal notranslate"><span class="pre">confusion_table()</span></code> function takes as first argument the
predicted labels, and second argument the true labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">confusion_table</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">Direction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>145</td>
      <td>141</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>457</td>
      <td>507</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The diagonal elements of the confusion matrix indicate correct
predictions, while the off-diagonals represent incorrect
predictions. Hence our model correctly predicted that the market would
go up on 507 days and that it would go down on 145 days, for a
total of 507 + 145 = 652 correct predictions. The <code class="docutils literal notranslate"><span class="pre">np.mean()</span></code>
function can be used to compute the fraction of days for which the
prediction was correct. In this case, logistic regression correctly
predicted the movement of the market 52.2% of the time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">507</span><span class="o">+</span><span class="mi">145</span><span class="p">)</span><span class="o">/</span><span class="mi">1250</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">Direction</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.5216, 0.5216)
</pre></div>
</div>
</div>
</div>
<p>At first glance, it appears that the logistic regression model is
working a little better than random guessing. However, this result is
misleading because we trained and tested the model on the same set of
1,250 observations. In other words, <span class="math notranslate nohighlight">\(100-52.2=47.8%\)</span> is the
<em>training</em> error  rate. As we have seen
previously, the training error rate is often overly optimistic — it
tends to underestimate the test error rate.  In
order to better assess the accuracy of the logistic regression model
in this setting, we can fit the model using part of the data, and
then examine how well it predicts the <em>held out</em> data.  This
will yield a more realistic error rate, in the sense that in practice
we will be interested in our model’s performance not on the data that
we used to fit the model, but rather on days in the future for which
the market’s movements are unknown.</p>
<p>To implement this strategy, we first create a Boolean vector
corresponding to the observations from 2001 through 2004. We  then
use this vector to create a held out data set of observations from
2005.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="p">(</span><span class="n">Smarket</span><span class="o">.</span><span class="n">Year</span> <span class="o">&lt;</span> <span class="mi">2005</span><span class="p">)</span>
<span class="n">Smarket_train</span> <span class="o">=</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
<span class="n">Smarket_test</span> <span class="o">=</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">train</span><span class="p">]</span>
<span class="n">Smarket_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(252, 9)
</pre></div>
</div>
</div>
</div>
<p>The object <code class="docutils literal notranslate"><span class="pre">train</span></code> is a vector of 1,250 elements, corresponding
to the observations in our data set. The elements of the vector that
correspond to observations that occurred before 2005 are set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>, whereas those that correspond to observations in 2005 are
set to <code class="docutils literal notranslate"><span class="pre">False</span></code>.  Hence <code class="docutils literal notranslate"><span class="pre">train</span></code> is a
<em>boolean</em>   array, since its
elements are <code class="docutils literal notranslate"><span class="pre">True</span></code> and <code class="docutils literal notranslate"><span class="pre">False</span></code>.  Boolean arrays can be used
to obtain a subset of the rows or columns of a data frame
using the <code class="docutils literal notranslate"><span class="pre">loc</span></code> method. For instance,
the command <code class="docutils literal notranslate"><span class="pre">Smarket.loc[train]</span></code> would pick out a submatrix of the
stock market data set, corresponding only to the dates before 2005,
since those are the ones for which the elements of <code class="docutils literal notranslate"><span class="pre">train</span></code> are
<code class="docutils literal notranslate"><span class="pre">True</span></code>.  The <code class="docutils literal notranslate"><span class="pre">~</span></code> symbol can be used to negate all of the
elements of a Boolean vector. That is, <code class="docutils literal notranslate"><span class="pre">~train</span></code> is a vector
similar to <code class="docutils literal notranslate"><span class="pre">train</span></code>, except that the elements that are <code class="docutils literal notranslate"><span class="pre">True</span></code>
in <code class="docutils literal notranslate"><span class="pre">train</span></code> get swapped to <code class="docutils literal notranslate"><span class="pre">False</span></code> in <code class="docutils literal notranslate"><span class="pre">~train</span></code>, and vice versa.
Therefore, <code class="docutils literal notranslate"><span class="pre">Smarket.loc[~train]</span></code> yields a
subset of the rows of the data frame
of the stock market data containing only the observations for which
<code class="docutils literal notranslate"><span class="pre">train</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>.
The output above indicates that there are 252 such
observations.</p>
<p>We now fit a logistic regression model using only the subset of the
observations that correspond to dates before 2005. We then obtain predicted probabilities of the
stock market going up for each of the days in our test set — that is,
for the days in 2005.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">train</span><span class="p">]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">train</span><span class="p">]</span>
<span class="n">glm_train</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span>
                   <span class="n">X_train</span><span class="p">,</span>
                   <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">glm_train</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that we have trained and tested our model on two completely
separate data sets: training was performed using only the dates before
2005, and testing was performed using only the dates in 2005.</p>
<p>Finally, we compare the predictions for 2005 to the
actual movements of the market over that time period.
We will first store the test and training labels (recall <code class="docutils literal notranslate"><span class="pre">y_test</span></code> is binary).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="n">Smarket</span><span class="o">.</span><span class="n">Direction</span>
<span class="n">L_train</span><span class="p">,</span> <span class="n">L_test</span> <span class="o">=</span> <span class="n">D</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">D</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">train</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now we threshold the
fitted probability at 50% to form
our predicted labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;Down&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">252</span><span class="p">)</span>
<span class="n">labels</span><span class="p">[</span><span class="n">probs</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Up&#39;</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>77</td>
      <td>97</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>34</td>
      <td>44</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The test accuracy is about 48% while the error rate is about 52%</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="n">L_test</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.4801587301587302, 0.5198412698412699)
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">!=</span></code> notation means <em>not equal to</em>, and so the last command
computes the test set error rate. The results are rather
disappointing: the test error rate is 52%, which is worse than
random guessing! Of course this result is not all that surprising,
given that one would not generally expect to be able to use previous
days’ returns to predict future market performance. (After all, if it
were possible to do so, then the authors of this book would be out
striking it rich rather than writing a statistics textbook.)</p>
<p>We recall that the logistic regression model had very underwhelming
<em>p</em>-values associated with all of the predictors, and that the
smallest <em>p</em>-value, though not very small, corresponded to
<code class="docutils literal notranslate"><span class="pre">Lag1</span></code>. Perhaps by removing the variables that appear not to be
helpful in predicting  <code class="docutils literal notranslate"><span class="pre">Direction</span></code>, we can obtain a more
effective model. After all, using predictors that have no relationship
with the response tends to cause a deterioration in the test error
rate (since such predictors cause an increase in variance without a
corresponding decrease in bias), and so removing such predictors may
in turn yield an improvement.  Below we refit the logistic
regression using just  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  and  <code class="docutils literal notranslate"><span class="pre">Lag2</span></code>, which seemed to
have the highest predictive power in the original logistic regression
model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MS</span><span class="p">([</span><span class="s1">&#39;Lag1&#39;</span><span class="p">,</span> <span class="s1">&#39;Lag2&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Smarket</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Smarket</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">train</span><span class="p">]</span>
<span class="n">glm_train</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span>
                   <span class="n">X_train</span><span class="p">,</span>
                   <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Binomial</span><span class="p">())</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">glm_train</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">exog</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;Down&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">252</span><span class="p">)</span>
<span class="n">labels</span><span class="p">[</span><span class="n">probs</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Up&#39;</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>35</td>
      <td>35</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>76</td>
      <td>106</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s evaluate the overall accuracy as well as the accuracy within the days when
logistic regression predicts an increase.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">35</span><span class="o">+</span><span class="mi">106</span><span class="p">)</span><span class="o">/</span><span class="mi">252</span><span class="p">,</span><span class="mi">106</span><span class="o">/</span><span class="p">(</span><span class="mi">106</span><span class="o">+</span><span class="mi">76</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.5595238095238095, 0.5824175824175825)
</pre></div>
</div>
</div>
</div>
<p>Now the results appear to be a little better: 56% of the daily
movements have been correctly predicted. It is worth noting that in
this case, a much simpler strategy of predicting that the market will
increase every day will also be correct 56% of the time! Hence, in
terms of overall error rate, the logistic regression method is no
better than the naive approach. However, the confusion matrix
shows that on days when logistic regression predicts an increase in
the market, it has a 58% accuracy rate. This suggests a possible
trading strategy of buying on days when the model predicts an
increasing market, and avoiding trades on days when a decrease is
predicted. Of course one would need to investigate more carefully
whether this small improvement was real or just due to random chance.</p>
<p>Suppose that we want to predict the returns associated with particular
values of  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  and  <code class="docutils literal notranslate"><span class="pre">Lag2</span></code>. In particular, we want to
predict  <code class="docutils literal notranslate"><span class="pre">Direction</span></code>  on a day when  <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  and
<code class="docutils literal notranslate"><span class="pre">Lag2</span></code>  equal <span class="math notranslate nohighlight">\(1.2\)</span> and <span class="math notranslate nohighlight">\(1.1\)</span>, respectively, and on a day when they
equal <span class="math notranslate nohighlight">\(1.5\)</span> and <span class="math notranslate nohighlight">\(-0.8\)</span>.  We do this using the <code class="docutils literal notranslate"><span class="pre">predict()</span></code>
function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">newdata</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Lag1&#39;</span><span class="p">:[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span>
                        <span class="s1">&#39;Lag2&#39;</span><span class="p">:[</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">]});</span>
<span class="n">newX</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">newdata</span><span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">newX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    0.479146
1    0.496094
dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="linear-discriminant-analysis">
<h2>Linear Discriminant Analysis<a class="headerlink" href="#linear-discriminant-analysis" title="Permalink to this heading">#</a></h2>
<p>We begin by performing LDA on the  <code class="docutils literal notranslate"><span class="pre">Smarket</span></code>  data, using the function
<code class="docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis()</span></code>, which we have abbreviated <code class="docutils literal notranslate"><span class="pre">LDA()</span></code>. We
fit the model using only the observations before 2005.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">(</span><span class="n">store_covariance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Since the <code class="docutils literal notranslate"><span class="pre">LDA</span></code> estimator automatically
adds an intercept, we should remove the column corresponding to the
intercept in both <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">X_test</span></code>. We can also directly
use the labels rather than the Boolean vectors <code class="docutils literal notranslate"><span class="pre">y_train</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="p">[</span><span class="n">M</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;intercept&#39;</span><span class="p">])</span>
                   <span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">]]</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">L_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearDiscriminantAnalysis</label><div class="sk-toggleable__content"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div></div></div>
</div>
<p>Here we have used the list comprehensions introduced
in Section 3.6.4. Looking at our first line above, we see that the right-hand side is a list
of length two. This is because the code <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">M</span> <span class="pre">in</span> <span class="pre">[X_train,</span> <span class="pre">X_test]</span></code> iterates over a list
of length two. While here we loop over a list,
the list comprehension method works when looping over any iterable object.
We then apply the <code class="docutils literal notranslate"><span class="pre">drop()</span></code> method to each element in the iteration, collecting
the result in a list. The left-hand side tells <code class="docutils literal notranslate"><span class="pre">Python</span></code> to unpack this list
of length two, assigning its elements to the variables <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">X_test</span></code>. Of course,
this overwrites the previous values of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">X_test</span></code>.</p>
<p>Having fit the model, we can extract the means in the two classes with the <code class="docutils literal notranslate"><span class="pre">means_</span></code> attribute. These are the average of each predictor within each class, and
are used by LDA as estimates of <span class="math notranslate nohighlight">\(\mu_k\)</span>.  These suggest that there is
a tendency for the previous 2 days’ returns to be negative on days
when the market increases, and a tendency for the previous days’
returns to be positive on days when the market declines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">means_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.04279022,  0.03389409],
       [-0.03954635, -0.03132544]])
</pre></div>
</div>
</div>
</div>
<p>The estimated prior probabilities are stored in the <code class="docutils literal notranslate"><span class="pre">priors_</span></code> attribute.
The package <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> typically uses this trailing <code class="docutils literal notranslate"><span class="pre">_</span></code> to denote
a quantity estimated when using the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method. We can be sure of which
entry corresponds to which label by looking at the <code class="docutils literal notranslate"><span class="pre">classes_</span></code> attribute.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Down&#39;, &#39;Up&#39;], dtype=&#39;&lt;U4&#39;)
</pre></div>
</div>
</div>
</div>
<p>The LDA output indicates that <span class="math notranslate nohighlight">\(\hat\pi_{Down}=0.492\)</span> and
<span class="math notranslate nohighlight">\(\hat\pi_{Up}=0.508\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">priors_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.49198397, 0.50801603])
</pre></div>
</div>
</div>
</div>
<p>The linear discriminant vectors can be found in the <code class="docutils literal notranslate"><span class="pre">scalings_</span></code> attribute:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">scalings_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-0.64201904],
       [-0.51352928]])
</pre></div>
</div>
</div>
</div>
<p>These values provide the linear combination of <code class="docutils literal notranslate"><span class="pre">Lag1</span></code>  and <code class="docutils literal notranslate"><span class="pre">Lag2</span></code>  that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of <span class="math notranslate nohighlight">\(X=x\)</span> in (4.24).
If <span class="math notranslate nohighlight">\(-0.64\times `Lag1`  - 0.51 \times `Lag2` \)</span> is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_pred</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As we observed in our comparison of classification methods
(Section 4.5),  the LDA and logistic
regression predictions are almost identical.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">confusion_table</span><span class="p">(</span><span class="n">lda_pred</span><span class="p">,</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>35</td>
      <td>35</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>76</td>
      <td>106</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can also estimate the
probability of each class for
each point in a training set. Applying a 50% threshold to the posterior probabilities of
being in class 1 allows us to
recreate the predictions contained in <code class="docutils literal notranslate"><span class="pre">lda_pred</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda_prob</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span>
       <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">lda_prob</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;Up&#39;</span><span class="p">,</span><span class="s1">&#39;Down&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="n">lda_pred</span>
       <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Above, we used the <code class="docutils literal notranslate"><span class="pre">np.where()</span></code>  function that
creates an array with value <code class="docutils literal notranslate"><span class="pre">'Up'</span></code> for indices where
the second column of <code class="docutils literal notranslate"><span class="pre">lda_prob</span></code> (the estimated
posterior probability of <code class="docutils literal notranslate"><span class="pre">'Up'</span></code>) is greater than 0.5.
For problems with more than two classes the labels are chosen as the class whose posterior probability is highest:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span>
       <span class="p">[</span><span class="n">lda</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lda_prob</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">==</span> <span class="n">lda_pred</span>
       <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>If we wanted to use a posterior probability threshold other than
50% in order to make predictions, then we could easily do so. For
instance, suppose that we wish to predict a market decrease only if we
are very certain that the market will indeed decrease on that
day — say, if the posterior probability is at least 90%.
We know that the first column of <code class="docutils literal notranslate"><span class="pre">lda_prob</span></code> corresponds to the
label <code class="docutils literal notranslate"><span class="pre">Down</span></code> after having checked the <code class="docutils literal notranslate"><span class="pre">classes_</span></code> attribute, hence we use
the column index 0 rather than 1 as we did above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lda_prob</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
</pre></div>
</div>
</div>
</div>
<p>No days in 2005 meet that threshold! In fact, the greatest posterior
probability of decrease in all of 2005 was 52.02%.</p>
<p>The LDA classifier above is the first classifier from the
<code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library. We will use several other objects
from this library. The objects
follow a common structure that simplifies tasks such as cross-validation,
which we will see in Chapter 5. Specifically,
the methods first create a generic classifier without
referring to any data. This classifier is then fit
to data with the <code class="docutils literal notranslate"><span class="pre">fit()</span></code>  method and predictions are
always produced with the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method. This pattern
of first instantiating the classifier, followed by fitting it, and
then producing predictions is an explicit design choice of <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. This uniformity
makes it possible to cleanly copy the classifier so that it can be fit
on different data; e.g. different training sets arising in cross-validation.
This standard pattern also allows for a predictable formation of workflows.</p>
</section>
<section id="quadratic-discriminant-analysis">
<h2>Quadratic Discriminant Analysis<a class="headerlink" href="#quadratic-discriminant-analysis" title="Permalink to this heading">#</a></h2>
<p>We will now fit a QDA model to the  <code class="docutils literal notranslate"><span class="pre">Smarket</span></code>  data. QDA is
implemented via
<code class="docutils literal notranslate"><span class="pre">QuadraticDiscriminantAnalysis()</span></code>
in the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> package, which we abbreviate to <code class="docutils literal notranslate"><span class="pre">QDA()</span></code>.
The syntax is very similar to <code class="docutils literal notranslate"><span class="pre">LDA()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qda</span> <span class="o">=</span> <span class="n">QDA</span><span class="p">(</span><span class="n">store_covariance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">qda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">L_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">QuadraticDiscriminantAnalysis</label><div class="sk-toggleable__content"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div></div></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">QDA()</span></code> function will again compute <code class="docutils literal notranslate"><span class="pre">means_</span></code> and <code class="docutils literal notranslate"><span class="pre">priors_</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qda</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span> <span class="n">qda</span><span class="o">.</span><span class="n">priors_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[ 0.04279022,  0.03389409],
        [-0.03954635, -0.03132544]]),
 array([0.49198397, 0.50801603]))
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">QDA()</span></code> classifier will estimate one covariance per class. Here is the
estimated covariance in the first class:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qda</span><span class="o">.</span><span class="n">covariance_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.50662277, -0.03924806],
       [-0.03924806,  1.53559498]])
</pre></div>
</div>
</div>
</div>
<p>The output contains the group means. But it does not contain the
coefficients of the linear discriminants, because the QDA classifier
involves a quadratic, rather than a linear, function of the
predictors. The <code class="docutils literal notranslate"><span class="pre">predict()</span></code>  function works in exactly the
same fashion as for LDA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qda_pred</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">qda_pred</span><span class="p">,</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>30</td>
      <td>20</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>81</td>
      <td>121</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Interestingly, the QDA predictions are accurate almost 60% of the
time, even though the 2005 data was not used to fit the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">qda_pred</span> <span class="o">==</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5992063492063492
</pre></div>
</div>
</div>
</div>
<p>This level of accuracy is quite impressive for stock market data, which is
known to be quite hard to model accurately.  This suggests that the
quadratic form assumed by QDA may capture the true relationship more
accurately than the linear forms assumed by LDA and logistic
regression.  However, we recommend evaluating this method’s
performance on a larger test set before betting that this approach
will consistently beat the market!</p>
</section>
<section id="naive-bayes">
<h2>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this heading">#</a></h2>
<p>Next, we fit a naive Bayes model to the <code class="docutils literal notranslate"><span class="pre">Smarket</span></code> data. The syntax is
similar to that of <code class="docutils literal notranslate"><span class="pre">LDA()</span></code> and <code class="docutils literal notranslate"><span class="pre">QDA()</span></code>. By
default, this implementation <code class="docutils literal notranslate"><span class="pre">GaussianNB()</span></code> of the naive Bayes classifier models each
quantitative feature using a Gaussian distribution. However, a kernel
density method can also be used to estimate the distributions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NB</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">NB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">L_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianNB</label><div class="sk-toggleable__content"><pre>GaussianNB()</pre></div></div></div></div></div></div></div>
</div>
<p>The classes are stored as <code class="docutils literal notranslate"><span class="pre">classes_</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NB</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Down&#39;, &#39;Up&#39;], dtype=&#39;&lt;U4&#39;)
</pre></div>
</div>
</div>
</div>
<p>The class prior probabilities are stored in the <code class="docutils literal notranslate"><span class="pre">class_prior_</span></code> attribute.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NB</span><span class="o">.</span><span class="n">class_prior_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.49198397, 0.50801603])
</pre></div>
</div>
</div>
</div>
<p>The parameters of the features can be found in the <code class="docutils literal notranslate"><span class="pre">theta_</span></code> and <code class="docutils literal notranslate"><span class="pre">var_</span></code> attributes. The number of rows
is equal to the number of classes, while the number of columns is equal to the number of features.
We see below that the mean for feature <code class="docutils literal notranslate"><span class="pre">Lag1</span></code> in the <code class="docutils literal notranslate"><span class="pre">Down</span></code> class is 0.043.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NB</span><span class="o">.</span><span class="n">theta_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.04279022,  0.03389409],
       [-0.03954635, -0.03132544]])
</pre></div>
</div>
</div>
</div>
<p>Its variance is 1.503.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NB</span><span class="o">.</span><span class="n">var_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1.50355429, 1.53246749],
       [1.51401364, 1.48732877]])
</pre></div>
</div>
</div>
</div>
<p>How do we know the names of these attributes? We use <code class="docutils literal notranslate"><span class="pre">NB?</span></code> (or \textR?NB}).</p>
<p>We can easily verify the mean computation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">[</span><span class="n">L_train</span> <span class="o">==</span> <span class="s1">&#39;Down&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Lag1    0.042790
Lag2    0.033894
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Similarly for the variance:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">[</span><span class="n">L_train</span> <span class="o">==</span> <span class="s1">&#39;Down&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Lag1    1.503554
Lag2    1.532467
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Since <code class="docutils literal notranslate"><span class="pre">NB()</span></code> is a classifier in the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> library, making predictions
uses the same syntax as for <code class="docutils literal notranslate"><span class="pre">LDA()</span></code> and <code class="docutils literal notranslate"><span class="pre">QDA()</span></code> above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_labels</span> <span class="o">=</span> <span class="n">NB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">nb_labels</span><span class="p">,</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>29</td>
      <td>20</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>82</td>
      <td>121</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Naive Bayes performs well on these data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA.</p>
<p>As for <code class="docutils literal notranslate"><span class="pre">LDA</span></code>, the <code class="docutils literal notranslate"><span class="pre">predict_proba()</span></code> method estimates the probability that each observation belongs to a particular class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NB</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.4873288 , 0.5126712 ],
       [0.47623584, 0.52376416],
       [0.46529531, 0.53470469],
       [0.47484469, 0.52515531],
       [0.49020587, 0.50979413]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="k-nearest-neighbors">
<h2>K-Nearest Neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Permalink to this heading">#</a></h2>
<p>We will now perform KNN using the <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier()</span></code> function. This function works similarly
to the other model-fitting functions that we have
encountered thus far.</p>
<p>As is the
case for LDA and QDA, we fit the classifier
using the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method. New
predictions are formed using the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method
of the object returned by <code class="docutils literal notranslate"><span class="pre">fit()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn1</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">knn1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">L_train</span><span class="p">)</span>
<span class="n">knn1_pred</span> <span class="o">=</span> <span class="n">knn1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">knn1_pred</span><span class="p">,</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>Down</th>
      <th>Up</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Down</th>
      <td>43</td>
      <td>58</td>
    </tr>
    <tr>
      <th>Up</th>
      <td>68</td>
      <td>83</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results using <span class="math notranslate nohighlight">\(K=1\)</span> are not very good, since only <span class="math notranslate nohighlight">\(50%\)</span> of the
observations are correctly predicted. Of course, it may be that <span class="math notranslate nohighlight">\(K=1\)</span>
results in an overly-flexible fit to the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">83</span><span class="o">+</span><span class="mi">43</span><span class="p">)</span><span class="o">/</span><span class="mi">252</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">knn1_pred</span> <span class="o">==</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.5, 0.5)
</pre></div>
</div>
</div>
</div>
<p>Below, we repeat the
analysis using <span class="math notranslate nohighlight">\(K=3\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn3</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn3_pred</span> <span class="o">=</span> <span class="n">knn3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">L_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">knn3_pred</span> <span class="o">==</span> <span class="n">L_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.5317460317460317
</pre></div>
</div>
</div>
</div>
<p>The results have improved slightly. But increasing <em>K</em> further
provides no further improvements. It appears that for these data, and this train/test split,
QDA gives the best results of the methods that we have examined so
far.</p>
<p>KNN does not perform well on the <code class="docutils literal notranslate"><span class="pre">Smarket</span></code>  data, but it often does provide impressive results. As an example we will apply the KNN approach to the <code class="docutils literal notranslate"><span class="pre">Caravan</span></code>  data set, which is part of the <code class="docutils literal notranslate"><span class="pre">ISLP</span></code> library.  This data set includes 85
predictors that measure demographic characteristics for 5,822
individuals. The response variable is  <code class="docutils literal notranslate"><span class="pre">Purchase</span></code>, which
indicates whether or not a given individual purchases a caravan
insurance policy. In this data set, only 6% of people purchased
caravan insurance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Caravan</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s1">&#39;Caravan&#39;</span><span class="p">)</span>
<span class="n">Purchase</span> <span class="o">=</span> <span class="n">Caravan</span><span class="o">.</span><span class="n">Purchase</span>
<span class="n">Purchase</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Purchase
No     5474
Yes     348
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>The method <code class="docutils literal notranslate"><span class="pre">value_counts()</span></code> takes a <code class="docutils literal notranslate"><span class="pre">pd.Series</span></code> or <code class="docutils literal notranslate"><span class="pre">pd.DataFrame</span></code> and returns
a <code class="docutils literal notranslate"><span class="pre">pd.Series</span></code> with the corresponding counts
for each unique element. In this case <code class="docutils literal notranslate"><span class="pre">Purchase</span></code> has only <code class="docutils literal notranslate"><span class="pre">Yes</span></code> and <code class="docutils literal notranslate"><span class="pre">No</span></code> values
and returns how many values of each there are.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">348</span> <span class="o">/</span> <span class="mi">5822</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.05977327378907592
</pre></div>
</div>
</div>
</div>
<p>Our features will include all columns except <code class="docutils literal notranslate"><span class="pre">Purchase</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_df</span> <span class="o">=</span> <span class="n">Caravan</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Purchase&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Because the KNN classifier predicts the class of a given test
observation by identifying the observations that are nearest to it,
the scale of the variables matters. Any variables that are on a large
scale will have a much larger effect on the <em>distance</em> between
the observations, and hence on the KNN classifier, than variables that
are on a small scale. For instance, imagine a data set that contains
two variables,  <code class="docutils literal notranslate"><span class="pre">salary</span></code>  and  <code class="docutils literal notranslate"><span class="pre">age</span></code>  (measured in dollars
and years, respectively). As far as KNN is concerned, a difference of
1,000 USD in salary is enormous compared to a difference of 50 years in
age. Consequently,  <code class="docutils literal notranslate"><span class="pre">salary</span></code>  will drive the KNN classification
results, and  <code class="docutils literal notranslate"><span class="pre">age</span></code>  will have almost no effect. This is contrary
to our intuition that a salary difference of 1,000 USD is quite small
compared to an age difference of 50 years.  Furthermore, the
importance of scale to the KNN classifier leads to another issue: if
we measured  <code class="docutils literal notranslate"><span class="pre">salary</span></code>  in Japanese yen, or if we measured
<code class="docutils literal notranslate"><span class="pre">age</span></code>  in minutes, then we’d get quite different classification
results from what we get if these two variables are measured in
dollars and years.</p>
<p>A good way to handle this problem is to <em>standardize</em>  the data so that all variables are
given a mean of zero and a standard deviation of one. Then all
variables will be on a comparable scale. This is accomplished
using
the <code class="docutils literal notranslate"><span class="pre">StandardScaler()</span></code>
transformation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(</span><span class="n">with_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">with_std</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The argument <code class="docutils literal notranslate"><span class="pre">with_mean</span></code> indicates whether or not
we should subtract the mean, while <code class="docutils literal notranslate"><span class="pre">with_std</span></code> indicates
whether or not we should scale the columns to have standard
deviation of 1 or not. Finally, the argument <code class="docutils literal notranslate"><span class="pre">copy=True</span></code>
indicates that we will always copy data, rather than
trying to do calculations in place where possible.</p>
<p>This transformation can be fit
and then applied to arbitrary data. In the first line
below, the parameters for the scaling are computed and
stored in <code class="docutils literal notranslate"><span class="pre">scaler</span></code>, while the second line actually
constructs the standardized set of features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">feature_df</span><span class="p">)</span>
<span class="n">X_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">feature_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now every column of <code class="docutils literal notranslate"><span class="pre">feature_std</span></code> below has a standard deviation of
one and a mean of zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                 <span class="n">X_std</span><span class="p">,</span>
                 <span class="n">columns</span><span class="o">=</span><span class="n">feature_df</span><span class="o">.</span><span class="n">columns</span><span class="p">);</span>
<span class="n">feature_std</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MOSTYPE     1.000086
MAANTHUI    1.000086
MGEMOMV     1.000086
MGEMLEEF    1.000086
MOSHOOFD    1.000086
              ...   
AZEILPL     1.000086
APLEZIER    1.000086
AFIETS      1.000086
AINBOED     1.000086
ABYSTAND    1.000086
Length: 85, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Notice that the standard deviations are not quite <span class="math notranslate nohighlight">\(1\)</span> here; this is again due to some procedures using the <span class="math notranslate nohighlight">\(1/n\)</span> convention for variances (in this case <code class="docutils literal notranslate"><span class="pre">scaler()</span></code>), while others use <span class="math notranslate nohighlight">\(1/(n-1)\)</span> (the <code class="docutils literal notranslate"><span class="pre">std()</span></code> method). See the footnote on page 198.
In this case it does not matter, as long as the variables are all on the same scale.</p>
<p>Using the function <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code>  we now split the observations into a test set,
containing 1000 observations, and a training set containing the remaining
observations. The argument <code class="docutils literal notranslate"><span class="pre">random_state=0</span></code> ensures that we get
the same split each time we rerun the code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span>
 <span class="n">X_test</span><span class="p">,</span>
 <span class="n">y_train</span><span class="p">,</span>
 <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">feature_std</span><span class="p">,</span>
                            <span class="n">Purchase</span><span class="p">,</span>
                            <span class="n">test_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                            <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">?train_test_split</span></code> reveals that the non-keyword arguments can be <code class="docutils literal notranslate"><span class="pre">lists</span></code>, <code class="docutils literal notranslate"><span class="pre">arrays</span></code>, <code class="docutils literal notranslate"><span class="pre">pandas</span> <span class="pre">dataframes</span></code> etc that all have the same length (<code class="docutils literal notranslate"><span class="pre">shape[0]</span></code>) and hence are <em>indexable</em>. In this case they are the dataframe <code class="docutils literal notranslate"><span class="pre">feature_std</span></code> and the response variable <code class="docutils literal notranslate"><span class="pre">Purchase</span></code>.
We fit a KNN model on the training data using <span class="math notranslate nohighlight">\(K=1\)</span>,
and evaluate its performance on the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn1</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">knn1_pred</span> <span class="o">=</span> <span class="n">knn1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">!=</span> <span class="n">knn1_pred</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">!=</span> <span class="s2">&quot;No&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">57</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">knn1</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">knn1_pred</span> <span class="o">=</span> <span class="n">knn1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">!=</span> <span class="n">knn1_pred</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">!=</span> <span class="s2">&quot;No&quot;</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:246,</span> in <span class="ni">KNeighborsClassifier.predict</span><span class="nt">(self, X)</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span> <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_fit_method&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">245</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">246</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_method</span> <span class="o">==</span> <span class="s2">&quot;brute&quot;</span> <span class="ow">and</span> <span class="n">ArgKminClassMode</span><span class="o">.</span><span class="n">is_usable_for</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>         <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span>     <span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span>         <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs_2d_</span><span class="p">:</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:471,</span> in <span class="ni">ArgKminClassMode.is_usable_for</span><span class="nt">(cls, X, Y, metric)</span>
<span class="g g-Whitespace">    </span><span class="mi">448</span> <span class="nd">@classmethod</span>
<span class="g g-Whitespace">    </span><span class="mi">449</span> <span class="k">def</span> <span class="nf">is_usable_for</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Return True if the dispatcher can be used for the given parameters.</span>
<span class="g g-Whitespace">    </span><span class="mi">451</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">452</span><span class="sd">     Parameters</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">468</span><span class="sd">     True if the PairwiseDistancesReduction can be used, else False.</span>
<span class="g g-Whitespace">    </span><span class="mi">469</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">470</span>     <span class="k">return</span> <span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">471</span>         <span class="n">ArgKmin</span><span class="o">.</span><span class="n">is_usable_for</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">472</span>         <span class="c1"># TODO: Support CSR matrices.</span>
<span class="g g-Whitespace">    </span><span class="mi">473</span>         <span class="ow">and</span> <span class="ow">not</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">474</span>         <span class="ow">and</span> <span class="ow">not</span> <span class="n">issparse</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">475</span>         <span class="c1"># TODO: implement Euclidean specialization with GEMM.</span>
<span class="nn">    476         and metric not</span> in <span class="nt">(&quot;euclidean&quot;, &quot;sqeuclidean&quot;)</span>
<span class="g g-Whitespace">    </span><span class="mi">477</span>     <span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:115,</span> in <span class="ni">BaseDistancesReductionDispatcher.is_usable_for</span><span class="nt">(cls, X, Y, metric)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span> <span class="k">def</span> <span class="nf">is_valid_sparse_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">102</span>     <span class="k">return</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">103</span>         <span class="n">isspmatrix_csr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span>         <span class="ow">and</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">110</span>         <span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span>
<span class="g g-Whitespace">    </span><span class="mi">111</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">113</span> <span class="n">is_usable</span> <span class="o">=</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">114</span>     <span class="n">get_config</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;enable_cython_pairwise_dist&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">115</span>     <span class="ow">and</span> <span class="p">(</span><span class="n">is_numpy_c_ordered</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_valid_sparse_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">116</span>     <span class="ow">and</span> <span class="p">(</span><span class="n">is_numpy_c_ordered</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_valid_sparse_matrix</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">117</span>     <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">dtype</span>
<span class="nn">    118     and X.dtype</span> in <span class="nt">(np.float32, np.float64)</span>
<span class="nn">    119     and metric</span> in <span class="ni">cls.valid_metrics</span><span class="nt">()</span>
<span class="g g-Whitespace">    </span><span class="mi">120</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">122</span> <span class="k">return</span> <span class="n">is_usable</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:99,</span> in <span class="ni">BaseDistancesReductionDispatcher.is_usable_for.&lt;locals&gt;.is_numpy_c_ordered</span><span class="nt">(X)</span>
<span class="g g-Whitespace">     </span><span class="mi">98</span> <span class="k">def</span> <span class="nf">is_numpy_c_ordered</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">99</span>     <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;flags&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">c_contiguous</span>

<span class="ne">AttributeError</span>: &#39;Flags&#39; object has no attribute &#39;c_contiguous&#39;
</pre></div>
</div>
</div>
</div>
<p>The KNN error rate on the 1,000 test observations is about <span class="math notranslate nohighlight">\(11%\)</span>.
At first glance, this may appear to be fairly good. However, since
just over 6% of customers purchased insurance, we could get the error
rate down to almost 6% by always predicting <code class="docutils literal notranslate"><span class="pre">No</span></code> regardless of the
values of the predictors! This is known as the <em>null rate</em>.}</p>
<p>Suppose that there is some non-trivial cost to trying to sell
insurance to a given individual. For instance, perhaps a salesperson
must visit each potential customer.  If the company tries to sell
insurance to a random selection of customers, then the success rate
will be only 6%, which may be far too low given the costs
involved.  Instead, the company would like to try to sell insurance
only to customers who are likely to buy it. So the overall error rate
is not of interest. Instead, the fraction of individuals that are
correctly predicted to buy insurance is of interest.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">confusion_table</span><span class="p">(</span><span class="n">knn1_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ValueError</span><span class="g g-Whitespace">                                </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">58</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">confusion_table</span><span class="p">(</span><span class="n">knn1_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/ISLP/__init__.py:101,</span> in <span class="ni">confusion_table</span><span class="nt">(predicted_labels, true_labels)</span>
<span class="g g-Whitespace">     </span><span class="mi">93</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">94</span><span class="sd"> Return a data frame version of confusion </span>
<span class="g g-Whitespace">     </span><span class="mi">95</span><span class="sd"> matrix with rows given by predicted label</span>
<span class="g g-Whitespace">     </span><span class="mi">96</span><span class="sd"> and columns the truth.</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">99</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">true_labels</span><span class="p">)</span> <span class="o">+</span>
<span class="g g-Whitespace">    </span><span class="mi">100</span>                           <span class="nb">list</span><span class="p">(</span><span class="n">predicted_labels</span><span class="p">)))</span>
<span class="ne">--&gt; </span><span class="mi">101</span> <span class="n">C</span> <span class="o">=</span> <span class="n">_confusion_matrix</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">102</span>                       <span class="n">predicted_labels</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">103</span>                       <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span> <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span> <span class="c1"># swap rows and columns</span>
<span class="g g-Whitespace">    </span><span class="mi">105</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211,</span> in <span class="ni">validate_params.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">205</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">206</span>     <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span>         <span class="n">skip_parameter_validation</span><span class="o">=</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span>             <span class="n">prefer_skip_nested_validation</span> <span class="ow">or</span> <span class="n">global_skip_validation</span>
<span class="g g-Whitespace">    </span><span class="mi">209</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span>     <span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">211</span>         <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">212</span> <span class="k">except</span> <span class="n">InvalidParameterError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">213</span>     <span class="c1"># When the function is just a wrapper around an estimator, we allow</span>
<span class="g g-Whitespace">    </span><span class="mi">214</span>     <span class="c1"># the function to delegate validation to the estimator, but we replace</span>
<span class="g g-Whitespace">    </span><span class="mi">215</span>     <span class="c1"># the name of the estimator by the name of the function in the error</span>
<span class="g g-Whitespace">    </span><span class="mi">216</span>     <span class="c1"># message to avoid confusion.</span>
<span class="g g-Whitespace">    </span><span class="mi">217</span>     <span class="n">msg</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">218</span>         <span class="sa">r</span><span class="s2">&quot;parameter of \w+ must be&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">219</span>         <span class="sa">f</span><span class="s2">&quot;parameter of </span><span class="si">{</span><span class="n">func</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2"> must be&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">220</span>         <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">221</span>     <span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:326,</span> in <span class="ni">confusion_matrix</span><span class="nt">(y_true, y_pred, labels, sample_weight, normalize)</span>
<span class="g g-Whitespace">    </span><span class="mi">231</span> <span class="nd">@validate_params</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">232</span>     <span class="p">{</span>
<span class="g g-Whitespace">    </span><span class="mi">233</span>         <span class="s2">&quot;y_true&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;array-like&quot;</span><span class="p">],</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">242</span>     <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">243</span> <span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Compute confusion matrix to evaluate the accuracy of a classification.</span>
<span class="g g-Whitespace">    </span><span class="mi">245</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">246</span><span class="sd">     By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">324</span><span class="sd">     (0, 2, 1, 1)</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">326</span>     <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">327</span>     <span class="k">if</span> <span class="n">y_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span>         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> is not supported&quot;</span> <span class="o">%</span> <span class="n">y_type</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:84,</span> in <span class="ni">_check_targets</span><span class="nt">(y_true, y_pred)</span>
<span class="g g-Whitespace">     </span><span class="mi">57</span> <span class="k">def</span> <span class="nf">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">58</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Check that y_true and y_pred belong to the same classification task.</span>
<span class="g g-Whitespace">     </span><span class="mi">59</span><span class="sd"> </span>
<span class="g g-Whitespace">     </span><span class="mi">60</span><span class="sd">     This converts multiclass or binary types to a common shape, and raises a</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span><span class="sd">     y_pred : array or indicator matrix</span>
<span class="g g-Whitespace">     </span><span class="mi">83</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="ne">---&gt; </span><span class="mi">84</span>     <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span>     <span class="n">type_true</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="s2">&quot;y_true&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">86</span>     <span class="n">type_pred</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="s2">&quot;y_pred&quot;</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:409,</span> in <span class="ni">check_consistent_length</span><span class="nt">(*arrays)</span>
<span class="g g-Whitespace">    </span><span class="mi">407</span> <span class="n">uniques</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">408</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">uniques</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">409</span>     <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">410</span>         <span class="s2">&quot;Found input variables with inconsistent numbers of samples: </span><span class="si">%r</span><span class="s2">&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">411</span>         <span class="o">%</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lengths</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">412</span>     <span class="p">)</span>

<span class="ne">ValueError</span>: Found input variables with inconsistent numbers of samples: [1000, 252]
</pre></div>
</div>
</div>
</div>
<p>It turns out that KNN with <span class="math notranslate nohighlight">\(K=1\)</span> does far better than random guessing
among the customers that are predicted to buy insurance. Among 62
such customers, 9, or 14.5%, actually do purchase insurance.
This is double the rate that one would obtain from random guessing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">9</span><span class="o">/</span><span class="p">(</span><span class="mi">53</span><span class="o">+</span><span class="mi">9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.14516129032258066
</pre></div>
</div>
</div>
</div>
<section id="tuning-parameters">
<h3>Tuning Parameters<a class="headerlink" href="#tuning-parameters" title="Permalink to this heading">#</a></h3>
<p>The number of neighbors in KNN is referred to as a <em>tuning parameter</em>, also referred to as a <em>hyperparameter</em>.
We do not know <em>a priori</em> what value to use. It is therefore of interest
to see how the classifier performs on test data as we vary these
parameters. This can be achieved with a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, described in Section 2.3.8.
Here we use a for loop to look at the accuracy of our classifier in the group predicted to purchase
insurance as we vary the number of neighbors from 1 to 5:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">K</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
    <span class="n">knn_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">confusion_table</span><span class="p">(</span><span class="n">knn_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">templ</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;K=</span><span class="si">{0:d}</span><span class="s1">: # predicted to rent: </span><span class="si">{1:&gt;2}</span><span class="s1">,&#39;</span> <span class="o">+</span>
            <span class="s1">&#39;  # who did rent </span><span class="si">{2:d}</span><span class="s1">, accuracy </span><span class="si">{3:.1%}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">did_rent</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Yes&#39;</span><span class="p">,</span><span class="s1">&#39;Yes&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">templ</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">K</span><span class="p">,</span>
          <span class="n">pred</span><span class="p">,</span>
          <span class="n">did_rent</span><span class="p">,</span>
          <span class="n">did_rent</span> <span class="o">/</span> <span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">AttributeError</span><span class="g g-Whitespace">                            </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">60</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="k">for</span> <span class="n">K</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span>     <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">3</span>     <span class="n">knn_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>     <span class="n">C</span> <span class="o">=</span> <span class="n">confusion_table</span><span class="p">(</span><span class="n">knn_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="n">templ</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;K=</span><span class="si">{0:d}</span><span class="s1">: # predicted to rent: </span><span class="si">{1:&gt;2}</span><span class="s1">,&#39;</span> <span class="o">+</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span>             <span class="s1">&#39;  # who did rent </span><span class="si">{2:d}</span><span class="s1">, accuracy </span><span class="si">{3:.1%}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:246,</span> in <span class="ni">KNeighborsClassifier.predict</span><span class="nt">(self, X)</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span> <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_fit_method&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">245</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">246</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_method</span> <span class="o">==</span> <span class="s2">&quot;brute&quot;</span> <span class="ow">and</span> <span class="n">ArgKminClassMode</span><span class="o">.</span><span class="n">is_usable_for</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>         <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span>     <span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span>         <span class="n">probabilities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span>         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs_2d_</span><span class="p">:</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:471,</span> in <span class="ni">ArgKminClassMode.is_usable_for</span><span class="nt">(cls, X, Y, metric)</span>
<span class="g g-Whitespace">    </span><span class="mi">448</span> <span class="nd">@classmethod</span>
<span class="g g-Whitespace">    </span><span class="mi">449</span> <span class="k">def</span> <span class="nf">is_usable_for</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;Return True if the dispatcher can be used for the given parameters.</span>
<span class="g g-Whitespace">    </span><span class="mi">451</span><span class="sd"> </span>
<span class="g g-Whitespace">    </span><span class="mi">452</span><span class="sd">     Parameters</span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">    </span><span class="mi">468</span><span class="sd">     True if the PairwiseDistancesReduction can be used, else False.</span>
<span class="g g-Whitespace">    </span><span class="mi">469</span><span class="sd">     &quot;&quot;&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">470</span>     <span class="k">return</span> <span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">471</span>         <span class="n">ArgKmin</span><span class="o">.</span><span class="n">is_usable_for</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">472</span>         <span class="c1"># TODO: Support CSR matrices.</span>
<span class="g g-Whitespace">    </span><span class="mi">473</span>         <span class="ow">and</span> <span class="ow">not</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">474</span>         <span class="ow">and</span> <span class="ow">not</span> <span class="n">issparse</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">475</span>         <span class="c1"># TODO: implement Euclidean specialization with GEMM.</span>
<span class="nn">    476         and metric not</span> in <span class="nt">(&quot;euclidean&quot;, &quot;sqeuclidean&quot;)</span>
<span class="g g-Whitespace">    </span><span class="mi">477</span>     <span class="p">)</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:115,</span> in <span class="ni">BaseDistancesReductionDispatcher.is_usable_for</span><span class="nt">(cls, X, Y, metric)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span> <span class="k">def</span> <span class="nf">is_valid_sparse_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">102</span>     <span class="k">return</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">103</span>         <span class="n">isspmatrix_csr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span>         <span class="ow">and</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">110</span>         <span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span>
<span class="g g-Whitespace">    </span><span class="mi">111</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">113</span> <span class="n">is_usable</span> <span class="o">=</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">114</span>     <span class="n">get_config</span><span class="p">()</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;enable_cython_pairwise_dist&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">115</span>     <span class="ow">and</span> <span class="p">(</span><span class="n">is_numpy_c_ordered</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_valid_sparse_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">116</span>     <span class="ow">and</span> <span class="p">(</span><span class="n">is_numpy_c_ordered</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_valid_sparse_matrix</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">117</span>     <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">dtype</span>
<span class="nn">    118     and X.dtype</span> in <span class="nt">(np.float32, np.float64)</span>
<span class="nn">    119     and metric</span> in <span class="ni">cls.valid_metrics</span><span class="nt">()</span>
<span class="g g-Whitespace">    </span><span class="mi">120</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">122</span> <span class="k">return</span> <span class="n">is_usable</span>

<span class="nn">File ~/work/isl/isl/.venv/lib/python3.10/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:99,</span> in <span class="ni">BaseDistancesReductionDispatcher.is_usable_for.&lt;locals&gt;.is_numpy_c_ordered</span><span class="nt">(X)</span>
<span class="g g-Whitespace">     </span><span class="mi">98</span> <span class="k">def</span> <span class="nf">is_numpy_c_ordered</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">99</span>     <span class="k">return</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="s2">&quot;flags&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">X</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">c_contiguous</span>

<span class="ne">AttributeError</span>: &#39;Flags&#39; object has no attribute &#39;c_contiguous&#39;
</pre></div>
</div>
</div>
</div>
<p>We see some variability —  the numbers for <code class="docutils literal notranslate"><span class="pre">K=4</span></code> are very different from the rest.</p>
</section>
<section id="comparison-to-logistic-regression">
<h3>Comparison to Logistic Regression<a class="headerlink" href="#comparison-to-logistic-regression" title="Permalink to this heading">#</a></h3>
<p>As a comparison, we can also fit a logistic regression model to the
data. This can also be done
with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, though by default it fits
something like the <em>ridge regression</em> version
of logistic regression, which we introduce in Chapter 6. This can
be modified by appropriately setting the argument <code class="docutils literal notranslate"><span class="pre">C</span></code> below. Its default
value is 1 but by setting it to a very large number, the algorithm converges to the same solution as the usual (unregularized)
logistic regression estimator discussed above.</p>
<p>Unlike the
<code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> package, <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> focuses less on
inference and more on classification. Hence,
the <code class="docutils literal notranslate"><span class="pre">summary</span></code> methods seen in <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code>
and our simplified version seen with <code class="docutils literal notranslate"><span class="pre">summarize</span></code> are not
generally available for the classifiers in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logit</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e10</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span>
<span class="n">logit</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">logit_pred</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">logit_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">logit_pred</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="s1">&#39;No&#39;</span><span class="p">)</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">logit_labels</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>No</th>
      <th>Yes</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>No</th>
      <td>933</td>
      <td>67</td>
    </tr>
    <tr>
      <th>Yes</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We used the argument <code class="docutils literal notranslate"><span class="pre">solver='liblinear'</span></code> above to
avoid a warning with the default solver which would indicate that
the algorithm does not converge.</p>
<p>If we use <span class="math notranslate nohighlight">\(0.5\)</span> as the predicted probability cut-off for the
classifier, then we have a problem: none of the test
observations are predicted to purchase insurance.  However, we are not required to use a
cut-off of <span class="math notranslate nohighlight">\(0.5\)</span>. If we instead predict a purchase any time the
predicted probability of purchase exceeds <span class="math notranslate nohighlight">\(0.25\)</span>, we get much better
results: we predict that 29 people will purchase insurance, and we are
correct for about 31% of these people. This is almost five times
better than random guessing!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logit_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">logit_pred</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mf">0.25</span><span class="p">,</span> <span class="s1">&#39;Yes&#39;</span><span class="p">,</span> <span class="s1">&#39;No&#39;</span><span class="p">)</span>
<span class="n">confusion_table</span><span class="p">(</span><span class="n">logit_labels</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Truth</th>
      <th>No</th>
      <th>Yes</th>
    </tr>
    <tr>
      <th>Predicted</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>No</th>
      <td>913</td>
      <td>58</td>
    </tr>
    <tr>
      <th>Yes</th>
      <td>20</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">9</span><span class="o">/</span><span class="p">(</span><span class="mi">20</span><span class="o">+</span><span class="mi">9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3103448275862069
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="linear-and-poisson-regression-on-the-bikeshare-data">
<h2>Linear and Poisson Regression on the Bikeshare Data<a class="headerlink" href="#linear-and-poisson-regression-on-the-bikeshare-data" title="Permalink to this heading">#</a></h2>
<p>Here we fit linear and  Poisson regression models to the <code class="docutils literal notranslate"><span class="pre">Bikeshare</span></code> data, as described in Section 4.6.
The response <code class="docutils literal notranslate"><span class="pre">bikers</span></code> measures the number of bike rentals per hour
in Washington, DC in the period 2010–2012.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Bike</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="s1">&#39;Bikeshare&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a peek at the dimensions and names of the variables in this dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Bike</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Bike</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((8645, 15),
 Index([&#39;season&#39;, &#39;mnth&#39;, &#39;day&#39;, &#39;hr&#39;, &#39;holiday&#39;, &#39;weekday&#39;, &#39;workingday&#39;,
        &#39;weathersit&#39;, &#39;temp&#39;, &#39;atemp&#39;, &#39;hum&#39;, &#39;windspeed&#39;, &#39;casual&#39;,
        &#39;registered&#39;, &#39;bikers&#39;],
       dtype=&#39;object&#39;))
</pre></div>
</div>
</div>
</div>
<section id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h3>
<p>We begin by fitting a linear regression model to the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">MS</span><span class="p">([</span><span class="s1">&#39;mnth&#39;</span><span class="p">,</span>
        <span class="s1">&#39;hr&#39;</span><span class="p">,</span>
        <span class="s1">&#39;workingday&#39;</span><span class="p">,</span>
        <span class="s1">&#39;temp&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weathersit&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Bike</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Bike</span><span class="p">[</span><span class="s1">&#39;bikers&#39;</span><span class="p">]</span>
<span class="n">M_lm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">summarize</span><span class="p">(</span><span class="n">M_lm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coef</th>
      <th>std err</th>
      <th>t</th>
      <th>P&gt;|t|</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>-27.2068</td>
      <td>6.715</td>
      <td>-4.052</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Aug]</th>
      <td>11.8181</td>
      <td>4.698</td>
      <td>2.515</td>
      <td>0.012</td>
    </tr>
    <tr>
      <th>mnth[Dec]</th>
      <td>5.0328</td>
      <td>4.280</td>
      <td>1.176</td>
      <td>0.240</td>
    </tr>
    <tr>
      <th>mnth[Feb]</th>
      <td>-34.5797</td>
      <td>4.575</td>
      <td>-7.558</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Jan]</th>
      <td>-41.4249</td>
      <td>4.972</td>
      <td>-8.331</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[July]</th>
      <td>3.8996</td>
      <td>5.003</td>
      <td>0.779</td>
      <td>0.436</td>
    </tr>
    <tr>
      <th>mnth[June]</th>
      <td>26.3938</td>
      <td>4.642</td>
      <td>5.686</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[March]</th>
      <td>-24.8735</td>
      <td>4.277</td>
      <td>-5.815</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[May]</th>
      <td>31.1322</td>
      <td>4.150</td>
      <td>7.501</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Nov]</th>
      <td>18.8851</td>
      <td>4.099</td>
      <td>4.607</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Oct]</th>
      <td>34.4093</td>
      <td>4.006</td>
      <td>8.589</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Sept]</th>
      <td>25.2534</td>
      <td>4.293</td>
      <td>5.883</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[1]</th>
      <td>-14.5793</td>
      <td>5.699</td>
      <td>-2.558</td>
      <td>0.011</td>
    </tr>
    <tr>
      <th>hr[2]</th>
      <td>-21.5791</td>
      <td>5.733</td>
      <td>-3.764</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[3]</th>
      <td>-31.1408</td>
      <td>5.778</td>
      <td>-5.389</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[4]</th>
      <td>-36.9075</td>
      <td>5.802</td>
      <td>-6.361</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[5]</th>
      <td>-24.1355</td>
      <td>5.737</td>
      <td>-4.207</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[6]</th>
      <td>20.5997</td>
      <td>5.704</td>
      <td>3.612</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[7]</th>
      <td>120.0931</td>
      <td>5.693</td>
      <td>21.095</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[8]</th>
      <td>223.6619</td>
      <td>5.690</td>
      <td>39.310</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[9]</th>
      <td>120.5819</td>
      <td>5.693</td>
      <td>21.182</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[10]</th>
      <td>83.8013</td>
      <td>5.705</td>
      <td>14.689</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[11]</th>
      <td>105.4234</td>
      <td>5.722</td>
      <td>18.424</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[12]</th>
      <td>137.2837</td>
      <td>5.740</td>
      <td>23.916</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[13]</th>
      <td>136.0359</td>
      <td>5.760</td>
      <td>23.617</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[14]</th>
      <td>126.6361</td>
      <td>5.776</td>
      <td>21.923</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[15]</th>
      <td>132.0865</td>
      <td>5.780</td>
      <td>22.852</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[16]</th>
      <td>178.5206</td>
      <td>5.772</td>
      <td>30.927</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[17]</th>
      <td>296.2670</td>
      <td>5.749</td>
      <td>51.537</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[18]</th>
      <td>269.4409</td>
      <td>5.736</td>
      <td>46.976</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[19]</th>
      <td>186.2558</td>
      <td>5.714</td>
      <td>32.596</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[20]</th>
      <td>125.5492</td>
      <td>5.704</td>
      <td>22.012</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[21]</th>
      <td>87.5537</td>
      <td>5.693</td>
      <td>15.378</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[22]</th>
      <td>59.1226</td>
      <td>5.689</td>
      <td>10.392</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[23]</th>
      <td>26.8376</td>
      <td>5.688</td>
      <td>4.719</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>workingday</th>
      <td>1.2696</td>
      <td>1.784</td>
      <td>0.711</td>
      <td>0.477</td>
    </tr>
    <tr>
      <th>temp</th>
      <td>157.2094</td>
      <td>10.261</td>
      <td>15.321</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>weathersit[cloudy/misty]</th>
      <td>-12.8903</td>
      <td>1.964</td>
      <td>-6.562</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>weathersit[heavy rain/snow]</th>
      <td>-109.7446</td>
      <td>76.667</td>
      <td>-1.431</td>
      <td>0.152</td>
    </tr>
    <tr>
      <th>weathersit[light rain/snow]</th>
      <td>-66.4944</td>
      <td>2.965</td>
      <td>-22.425</td>
      <td>0.000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>There are 24 levels in <code class="docutils literal notranslate"><span class="pre">hr</span></code> and 40 rows in all.
In <code class="docutils literal notranslate"><span class="pre">M_lm</span></code>, the first levels <code class="docutils literal notranslate"><span class="pre">hr[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">mnth[Jan]</span></code>  are treated
as the baseline values, and so no coefficient estimates are provided
for them: implicitly, their coefficient estimates are zero, and all
other levels are measured relative to these baselines. For example,
the Feb coefficient of <span class="math notranslate nohighlight">\(6.845\)</span> signifies that, holding all other
variables constant, there are on average about 7 more riders in
February than in January. Similarly there are about 16.5 more riders
in March than in January.</p>
<p>The results seen in Section 4.6.1
used a slightly different coding of the variables <code class="docutils literal notranslate"><span class="pre">hr</span></code> and <code class="docutils literal notranslate"><span class="pre">mnth</span></code>, as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hr_encode</span> <span class="o">=</span> <span class="n">contrast</span><span class="p">(</span><span class="s1">&#39;hr&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="n">mnth_encode</span> <span class="o">=</span> <span class="n">contrast</span><span class="p">(</span><span class="s1">&#39;mnth&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Refitting again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X2</span> <span class="o">=</span> <span class="n">MS</span><span class="p">([</span><span class="n">mnth_encode</span><span class="p">,</span>
         <span class="n">hr_encode</span><span class="p">,</span>
        <span class="s1">&#39;workingday&#39;</span><span class="p">,</span>
        <span class="s1">&#39;temp&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weathersit&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Bike</span><span class="p">)</span>
<span class="n">M2_lm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">S2</span> <span class="o">=</span> <span class="n">summarize</span><span class="p">(</span><span class="n">M2_lm</span><span class="p">)</span>
<span class="n">S2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coef</th>
      <th>std err</th>
      <th>t</th>
      <th>P&gt;|t|</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>intercept</th>
      <td>73.5974</td>
      <td>5.132</td>
      <td>14.340</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Jan]</th>
      <td>-46.0871</td>
      <td>4.085</td>
      <td>-11.281</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Feb]</th>
      <td>-39.2419</td>
      <td>3.539</td>
      <td>-11.088</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[March]</th>
      <td>-29.5357</td>
      <td>3.155</td>
      <td>-9.361</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[April]</th>
      <td>-4.6622</td>
      <td>2.741</td>
      <td>-1.701</td>
      <td>0.089</td>
    </tr>
    <tr>
      <th>mnth[May]</th>
      <td>26.4700</td>
      <td>2.851</td>
      <td>9.285</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[June]</th>
      <td>21.7317</td>
      <td>3.465</td>
      <td>6.272</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[July]</th>
      <td>-0.7626</td>
      <td>3.908</td>
      <td>-0.195</td>
      <td>0.845</td>
    </tr>
    <tr>
      <th>mnth[Aug]</th>
      <td>7.1560</td>
      <td>3.535</td>
      <td>2.024</td>
      <td>0.043</td>
    </tr>
    <tr>
      <th>mnth[Sept]</th>
      <td>20.5912</td>
      <td>3.046</td>
      <td>6.761</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Oct]</th>
      <td>29.7472</td>
      <td>2.700</td>
      <td>11.019</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>mnth[Nov]</th>
      <td>14.2229</td>
      <td>2.860</td>
      <td>4.972</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[0]</th>
      <td>-96.1420</td>
      <td>3.955</td>
      <td>-24.307</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[1]</th>
      <td>-110.7213</td>
      <td>3.966</td>
      <td>-27.916</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[2]</th>
      <td>-117.7212</td>
      <td>4.016</td>
      <td>-29.310</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[3]</th>
      <td>-127.2828</td>
      <td>4.081</td>
      <td>-31.191</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[4]</th>
      <td>-133.0495</td>
      <td>4.117</td>
      <td>-32.319</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[5]</th>
      <td>-120.2775</td>
      <td>4.037</td>
      <td>-29.794</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[6]</th>
      <td>-75.5424</td>
      <td>3.992</td>
      <td>-18.925</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[7]</th>
      <td>23.9511</td>
      <td>3.969</td>
      <td>6.035</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[8]</th>
      <td>127.5199</td>
      <td>3.950</td>
      <td>32.284</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[9]</th>
      <td>24.4399</td>
      <td>3.936</td>
      <td>6.209</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[10]</th>
      <td>-12.3407</td>
      <td>3.936</td>
      <td>-3.135</td>
      <td>0.002</td>
    </tr>
    <tr>
      <th>hr[11]</th>
      <td>9.2814</td>
      <td>3.945</td>
      <td>2.353</td>
      <td>0.019</td>
    </tr>
    <tr>
      <th>hr[12]</th>
      <td>41.1417</td>
      <td>3.957</td>
      <td>10.397</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[13]</th>
      <td>39.8939</td>
      <td>3.975</td>
      <td>10.036</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[14]</th>
      <td>30.4940</td>
      <td>3.991</td>
      <td>7.641</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[15]</th>
      <td>35.9445</td>
      <td>3.995</td>
      <td>8.998</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[16]</th>
      <td>82.3786</td>
      <td>3.988</td>
      <td>20.655</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[17]</th>
      <td>200.1249</td>
      <td>3.964</td>
      <td>50.488</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[18]</th>
      <td>173.2989</td>
      <td>3.956</td>
      <td>43.806</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[19]</th>
      <td>90.1138</td>
      <td>3.940</td>
      <td>22.872</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[20]</th>
      <td>29.4071</td>
      <td>3.936</td>
      <td>7.471</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>hr[21]</th>
      <td>-8.5883</td>
      <td>3.933</td>
      <td>-2.184</td>
      <td>0.029</td>
    </tr>
    <tr>
      <th>hr[22]</th>
      <td>-37.0194</td>
      <td>3.934</td>
      <td>-9.409</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>workingday</th>
      <td>1.2696</td>
      <td>1.784</td>
      <td>0.711</td>
      <td>0.477</td>
    </tr>
    <tr>
      <th>temp</th>
      <td>157.2094</td>
      <td>10.261</td>
      <td>15.321</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>weathersit[cloudy/misty]</th>
      <td>-12.8903</td>
      <td>1.964</td>
      <td>-6.562</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>weathersit[heavy rain/snow]</th>
      <td>-109.7446</td>
      <td>76.667</td>
      <td>-1.431</td>
      <td>0.152</td>
    </tr>
    <tr>
      <th>weathersit[light rain/snow]</th>
      <td>-66.4944</td>
      <td>2.965</td>
      <td>-22.425</td>
      <td>0.000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What is the difference between the two codings?  In <code class="docutils literal notranslate"><span class="pre">M2_lm</span></code>, a
coefficient estimate is reported for all but level <code class="docutils literal notranslate"><span class="pre">23</span></code> of <code class="docutils literal notranslate"><span class="pre">hr</span></code>
and level <code class="docutils literal notranslate"><span class="pre">Dec</span></code> of <code class="docutils literal notranslate"><span class="pre">mnth</span></code>. Importantly, in <code class="docutils literal notranslate"><span class="pre">M2_lm</span></code>, the (unreported) coefficient estimate
for the last level of <code class="docutils literal notranslate"><span class="pre">mnth</span></code> is not zero: instead, it equals the
negative of the sum of the coefficient estimates for all of the
other levels. Similarly, in <code class="docutils literal notranslate"><span class="pre">M2_lm</span></code>, the coefficient estimate
for the last level of <code class="docutils literal notranslate"><span class="pre">hr</span></code> is the negative of the sum of the
coefficient estimates for all of the other levels. This means that the
coefficients of <code class="docutils literal notranslate"><span class="pre">hr</span></code> and <code class="docutils literal notranslate"><span class="pre">mnth</span></code> in <code class="docutils literal notranslate"><span class="pre">M2_lm</span></code> will always sum
to zero, and can be interpreted as the difference from the mean
level. For example, the coefficient for January of <span class="math notranslate nohighlight">\(-46.087\)</span> indicates
that, holding all other variables constant, there are typically 46
fewer riders in January relative to the yearly average.</p>
<p>It is important to realize that the choice of coding really does not
matter, provided that we interpret the  model output correctly in light
of the coding used. For example, we see that the predictions from the
linear model are the same regardless of coding:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">M_lm</span><span class="o">.</span><span class="n">fittedvalues</span> <span class="o">-</span> <span class="n">M2_lm</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7.3550372999637e-21
</pre></div>
</div>
</div>
</div>
<p>The sum of squared differences is zero. We can also see this using the
<code class="docutils literal notranslate"><span class="pre">np.allclose()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">M_lm</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span> <span class="n">M2_lm</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>To reproduce the left-hand side of Figure 4.13
we must first obtain the coefficient estimates associated with
<code class="docutils literal notranslate"><span class="pre">mnth</span></code>. The coefficients for January through November can be obtained
directly from the <code class="docutils literal notranslate"><span class="pre">M2_lm</span></code> object. The coefficient for December
must be explicitly computed as the negative sum of all the other
months. We first extract all the coefficients for month from
the coefficients of <code class="docutils literal notranslate"><span class="pre">M2_lm</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef_month</span> <span class="o">=</span> <span class="n">S2</span><span class="p">[</span><span class="n">S2</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;mnth&#39;</span><span class="p">)][</span><span class="s1">&#39;coef&#39;</span><span class="p">]</span>
<span class="n">coef_month</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mnth[Jan]     -46.0871
mnth[Feb]     -39.2419
mnth[March]   -29.5357
mnth[April]    -4.6622
mnth[May]      26.4700
mnth[June]     21.7317
mnth[July]     -0.7626
mnth[Aug]       7.1560
mnth[Sept]     20.5912
mnth[Oct]      29.7472
mnth[Nov]      14.2229
Name: coef, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Next, we append <code class="docutils literal notranslate"><span class="pre">Dec</span></code> as the negative of the sum of all other months.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">months</span> <span class="o">=</span> <span class="n">Bike</span><span class="p">[</span><span class="s1">&#39;mnth&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">categories</span>
<span class="n">coef_month</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
                       <span class="n">coef_month</span><span class="p">,</span>
                       <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="o">-</span><span class="n">coef_month</span><span class="o">.</span><span class="n">sum</span><span class="p">()],</span>
                                  <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mnth[Dec]&#39;</span>
                                 <span class="p">])</span>
                       <span class="p">])</span>
<span class="n">coef_month</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mnth[Jan]     -46.0871
mnth[Feb]     -39.2419
mnth[March]   -29.5357
mnth[April]    -4.6622
mnth[May]      26.4700
mnth[June]     21.7317
mnth[July]     -0.7626
mnth[Aug]       7.1560
mnth[Sept]     20.5912
mnth[Oct]      29.7472
mnth[Nov]      14.2229
mnth[Dec]       0.3705
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Finally, to make the plot neater, we’ll just use the first letter of each month, which is the <span class="math notranslate nohighlight">\(6\)</span>th entry of each of
the labels in the index.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig_month</span><span class="p">,</span> <span class="n">ax_month</span> <span class="o">=</span> <span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">x_month</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">coef_month</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_month</span><span class="p">,</span> <span class="n">coef_month</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_month</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">coef_month</span><span class="o">.</span><span class="n">index</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/962b56875a381da345cb5e805a9e4165576b92818d6b62b98464bd8bbc8fe116.png" src="../../../_images/962b56875a381da345cb5e805a9e4165576b92818d6b62b98464bd8bbc8fe116.png" />
</div>
</div>
<p>Reproducing the  right-hand plot in Figure 4.13  follows a similar process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef_hr</span> <span class="o">=</span> <span class="n">S2</span><span class="p">[</span><span class="n">S2</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;hr&#39;</span><span class="p">)][</span><span class="s1">&#39;coef&#39;</span><span class="p">]</span>
<span class="n">coef_hr</span> <span class="o">=</span> <span class="n">coef_hr</span><span class="o">.</span><span class="n">reindex</span><span class="p">([</span><span class="s1">&#39;hr[</span><span class="si">{0}</span><span class="s1">]&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">23</span><span class="p">)])</span>
<span class="n">coef_hr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">coef_hr</span><span class="p">,</span>
                     <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="o">-</span><span class="n">coef_hr</span><span class="o">.</span><span class="n">sum</span><span class="p">()],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;hr[23]&#39;</span><span class="p">])</span>
                    <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We now make the hour plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig_hr</span><span class="p">,</span> <span class="n">ax_hr</span> <span class="o">=</span> <span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">x_hr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">coef_hr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hr</span><span class="p">,</span> <span class="n">coef_hr</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_hr</span><span class="p">[::</span><span class="mi">2</span><span class="p">])</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">24</span><span class="p">)[::</span><span class="mi">2</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hour&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ac1a29424e9d47e0350613cdfe267b07e94d1ff84a0167a6fc7bb92322bc5e06.png" src="../../../_images/ac1a29424e9d47e0350613cdfe267b07e94d1ff84a0167a6fc7bb92322bc5e06.png" />
</div>
</div>
</section>
<section id="poisson-regression">
<h3>Poisson Regression<a class="headerlink" href="#poisson-regression" title="Permalink to this heading">#</a></h3>
<p>Now we fit instead a Poisson regression model to the
<code class="docutils literal notranslate"><span class="pre">Bikeshare</span></code> data. Very little changes, except that we now use the
function <code class="docutils literal notranslate"><span class="pre">sm.GLM()</span></code> with the Poisson family specified:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">M_pois</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">Poisson</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We can plot the coefficients associated with <code class="docutils literal notranslate"><span class="pre">mnth</span></code> and <code class="docutils literal notranslate"><span class="pre">hr</span></code>, in order to reproduce  Figure 4.15. We first complete these coefficients as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">S_pois</span> <span class="o">=</span> <span class="n">summarize</span><span class="p">(</span><span class="n">M_pois</span><span class="p">)</span>
<span class="n">coef_month</span> <span class="o">=</span> <span class="n">S_pois</span><span class="p">[</span><span class="n">S_pois</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;mnth&#39;</span><span class="p">)][</span><span class="s1">&#39;coef&#39;</span><span class="p">]</span>
<span class="n">coef_month</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">coef_month</span><span class="p">,</span>
                        <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="o">-</span><span class="n">coef_month</span><span class="o">.</span><span class="n">sum</span><span class="p">()],</span>
                                   <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mnth[Dec]&#39;</span><span class="p">])])</span>
<span class="n">coef_hr</span> <span class="o">=</span> <span class="n">S_pois</span><span class="p">[</span><span class="n">S_pois</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;hr&#39;</span><span class="p">)][</span><span class="s1">&#39;coef&#39;</span><span class="p">]</span>
<span class="n">coef_hr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">coef_hr</span><span class="p">,</span>
                     <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="o">-</span><span class="n">coef_hr</span><span class="o">.</span><span class="n">sum</span><span class="p">()],</span>
                     <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;hr[23]&#39;</span><span class="p">])])</span>
</pre></div>
</div>
</div>
</div>
<p>The plotting is as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig_pois</span><span class="p">,</span> <span class="p">(</span><span class="n">ax_month</span><span class="p">,</span> <span class="n">ax_hr</span><span class="p">)</span> <span class="o">=</span> <span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_month</span><span class="p">,</span> <span class="n">coef_month</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x_month</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="n">l</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">coef_month</span><span class="o">.</span><span class="n">index</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_month</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_hr</span><span class="p">,</span> <span class="n">coef_hr</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">24</span><span class="p">)[::</span><span class="mi">2</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hour&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax_hr</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2196/3779905754.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator
  ax_hr.set_xticklabels(range(24)[::2], fontsize=20)
</pre></div>
</div>
<img alt="../../../_images/52e442ca136e14ba1861a84284a0b0b7ab6b4bba4a8727ea6483e6976c745262.png" src="../../../_images/52e442ca136e14ba1861a84284a0b0b7ab6b4bba4a8727ea6483e6976c745262.png" />
</div>
</div>
<p>We compare the fitted values of the two models.
The fitted values are stored in the <code class="docutils literal notranslate"><span class="pre">fittedvalues</span></code> attribute
returned by the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method for both the linear regression and the Poisson
fits. The linear predictors are stored as the attribute <code class="docutils literal notranslate"><span class="pre">lin_pred</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">M2_lm</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span>
           <span class="n">M_pois</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span>
           <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Linear Regression Fit&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Poisson Regression Fit&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axline</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
          <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/838011cc78e3d873b8a40a90a4dd702b97022645099c403a8206f9b3587c6703.png" src="../../../_images/838011cc78e3d873b8a40a90a4dd702b97022645099c403a8206f9b3587c6703.png" />
</div>
</div>
<p>The predictions from the Poisson regression model are correlated with
those from the linear model; however, the former are non-negative. As
a result the Poisson regression predictions tend to be larger than
those from the linear model for either very low or very high levels of
ridership.</p>
<p>In this section, we fit Poisson regression models using the <code class="docutils literal notranslate"><span class="pre">sm.GLM()</span></code> function with the argument
<code class="docutils literal notranslate"><span class="pre">family=sm.families.Poisson()</span></code>. Earlier in this lab we used the <code class="docutils literal notranslate"><span class="pre">sm.GLM()</span></code> function
with <code class="docutils literal notranslate"><span class="pre">family=sm.families.Binomial()</span></code> to perform logistic regression. Other
choices for the <code class="docutils literal notranslate"><span class="pre">family</span></code> argument can be used to fit other types
of GLMs. For instance, <code class="docutils literal notranslate"><span class="pre">family=sm.families.Gamma()</span></code> fits a Gamma regression
model.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/labs/notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Ch03-linreg-lab.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 3</p>
      </div>
    </a>
    <a class="right-next"
       href="Ch05-resample-lab.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 5</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 4</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-logistic-regression-lda-qda-and-knn">Lab: Logistic Regression, LDA, QDA, and KNN</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-stock-market-data">The Stock Market Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-discriminant-analysis">Quadratic Discriminant Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors">K-Nearest Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-parameters">Tuning Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-to-logistic-regression">Comparison to Logistic Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-and-poisson-regression-on-the-bikeshare-data">Linear and Poisson Regression on the Bikeshare Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#poisson-regression">Poisson Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Trevor Hastie et al.
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>